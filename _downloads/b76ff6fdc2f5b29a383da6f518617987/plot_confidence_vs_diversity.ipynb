{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nLowest confidence vs. KMeans sampling\n=====================================\n\nThis example shows the importance of diversity-based approaches using a\ntoy example where a very unlucky initialization makes lowest confidence\napproach underperform.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Those are the necessary imports and initializations\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\nfrom matplotlib.patches import Polygon\nimport numpy as np\n\nfrom sklearn.datasets.samples_generator import make_blobs\nfrom sklearn.svm import SVC\n\nfrom cardinal.uncertainty import ConfidenceSampler\nfrom cardinal.clustering import KMeansSampler\nfrom cardinal.batch import RankedBatchSampler\nfrom cardinal.random import RandomSampler\n\n\nnp.random.seed(7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We simulate data where samples of one of the class are scattered in 3\nblobs, one of them being far away from the two others. We also select an\ninitialization index where no sample from the far-away sample is initially\nselected. This will force the decision boundary to stay far from that cluster\nand thus \"trick\" the lowest confidence method.\n\nThe parameters of this experiment are :  \n\n* `n` is the number of points in the simulated data,\n* `batch_size` is the number of samples that will be annotated and added to\n  the training set at each iteration,\n* `n_iter` is the number of iterations in our simulation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n = 28\nbatch_size = 4\nn_iter = 5\n\nX, y = make_blobs(n_samples=n, centers=[(1, 0), (0, 1), (2, 2), (4, 0)],\n                  random_state=0, cluster_std=0.2)\n    \n# We select samples in clusters 0, 1 and 2. Cluster 3 will be ignored by uncertainty sampling\ninit_idx = [i for j in range(3) for i in np.where(y == j)[0][:2]]\ny[y > 1] = 1\n\nmodel = SVC(kernel='linear', C=1E10, probability=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This helper function plots our simulated points in red and blue. The one that\nare not in the training set are faded. We also plot the linear separation\nestimated by the SVM.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot(a, b, score, selected):\n\n    plt.xlabel('Accuracy {}%'.format(int(score * 100)), fontsize=10)\n\n    l_to_c = {0: 'tomato', 1:'royalblue'}\n\n    f = (lambda x: a * x + b)\n    x1, x2 = (np.min(X[:, 0]), np.max(X[:, 0]))\n    y1, y2 = (np.min(X[:, 1]), np.max(X[:, 1]))\n\n    # This code computes the coordinates of the background rectangles\n    # in order to have pretty prints.\n    p1, p2 = (x1, a * x1 + b), ((y1 - b) / a, y1)\n    p3, p4 = (x2, a * x2 + b), ((y2 - b) / a, y2)\n    p1, p2, p3, p4 = sorted([p1, p2, p3, p4])\n\n    corners = [(x1, y1), (x1, y2), (x2, y2), (x2, y1)]\n    dists = [f(x) - y for x, y in corners]\n    while dists[0] > 0 or dists[-1] < 0:\n        dists.append(dists.pop(0))\n        corners.append(corners.pop(0))\n    first_pos = next(i for i, x in enumerate(dists) if x > 0)\n    plt.gca().add_patch(Polygon(\n        [p3, p2] + corners[:first_pos], joinstyle='round',\n        facecolor=l_to_c[model.predict([corners[0]])[0]], alpha=0.2))\n    plt.gca().add_patch(Polygon(\n        [p2, p3] + corners[first_pos:], joinstyle='round',\n        facecolor=l_to_c[model.predict([corners[-1]])[0]], alpha=0.2))\n   \n    # Plot not selected first in low alpha, then selected\n    for l, s in [(0, False), (1, False), (0, True), (1, True)]:\n        alpha = 1. if s else 0.3\n        mask = np.logical_and(selected == s, l == y)\n        plt.scatter(X[mask, 0], X[mask, 1], c=l_to_c[l], alpha=alpha)\n        \n    # Plot the separation margin of the SVM\n    plt.plot(*zip(p2, p3), c='purple')\n    eps = 0.1\n    plt.gca().set_xlim(x1 - eps, x2 + eps)\n    plt.gca().set_ylim(y1 - eps, y2 + eps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Core Active Learning Experiment\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nAs presented in the introduction, this loop is the core of the active learning\nexperiment. At each iteration, the model learns on all labeled data to\nmeasure its performance. The model is then inspected to find out the samples\non which its confidence is the lowest. This is done through cardinal samplers.\n\nIn this experiment, we see that lowest confidence will explore the far-away\ncluster only once all other samples have been labeled. KMeans uses a more\nexploratory approach and select items in this cluster right away.\nIt is worth noticing that random sampling also have good exploration\nproperties.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "samplers = [\n    ('Lowest confidence', ConfidenceSampler(model, batch_size)),\n    ('KMeans', KMeansSampler(batch_size)),\n    ('Weighted KMeans', KMeansSampler(batch_size)),\n    ('Batch', RankedBatchSampler(batch_size)),\n    ('Random', RandomSampler(batch_size))\n]\n\nplt.figure(figsize=(10, 10))\n\nfor i, (sampler_name, sampler) in enumerate(samplers):\n    mask = np.zeros(n, dtype=bool)\n    indices = np.arange(n)\n    mask[init_idx] = True\n\n    for j in range(n_iter):\n        model.fit(X[mask], y[mask])\n        sampler.fit(X[mask], y[mask])\n        w = model.coef_[0]\n        \n        plt.subplot(len(samplers), n_iter, i * n_iter + j + 1)\n\n        if sampler_name == 'Batch':\n            # This is an SSL method that requires \n            weights = ConfidenceSampler(model, batch_size).score_samples(X)\n            weights[mask] = -1\n            selected = sampler.select_samples(X, samples_weights=weights)\n            mask[selected] = True\n        elif sampler_name == 'Weighted Kmeans':\n            weights = ConfidenceSampler(model, batch_size).score_samples(X[~mask])\n            selected = sampler.select_samples(X[~mask], samples_weights=weights)\n            mask[indices[~mask][selected]] = True\n        else:\n            selected = sampler.select_samples(X[~mask])\n            mask[indices[~mask][selected]] = True\n\n        if j == 0:\n            plt.ylabel(sampler_name)\n        plt.axis('tight')\n        plt.gca().set_xticks(())\n        plt.gca().set_yticks(())\n        if i == 0:\n            plt.gca().set_title('Iteration {}'.format(j), fontsize=10)\n\n        plot(-w[0] / w[1], - model.intercept_[0] / w[1], model.score(X, y),\n             mask.copy())\n\nplt.tight_layout()\nplt.subplots_adjust(top=0.86)\nplt.gcf().suptitle('Classification accuracy of random and uncertainty active learning on simulated data', fontsize=12)\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}