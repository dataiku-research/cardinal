{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nLowest confidence vs. Random sampling\n=====================================\n\nThe simplest way ton convince ourselves that active learning actually\nworks is to first test it on simulated data. In this example, we will\ngenerate a simple classification task and see how active learning allows\nont to converge faster toward the best result.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Those are the necessary imports and initialiaztion\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\nimport numpy as np\n\nfrom sklearn.datasets.samples_generator import make_blobs\nfrom sklearn.svm import SVC\n\nfrom cardinal.random import RandomSampler\nfrom cardinal.uncertainty import ConfidenceSampler\n\n\nnp.random.seed(8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Parameters of our experiment:\n* _n_ is the number of points in the sumulated data\n* _batch_size_ is the number of samples that will be annotated and added to\n  the training set at each iteration\n* _n_iter_ is the number of iterations in our simulation\n\nOur simulated data is composed of 2 clusters that are very close to each other\nbut linearly separable. We use as simple SVM classifier as it is a basic classifer.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n = 30\nbatch_size = 2\nn_iter = 6\n\nX, y = make_blobs(n_samples=n, centers=2,\n                  random_state=0, cluster_std=0.80)\n\nmodel = SVC(kernel='linear', C=1E10, probability=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This helper function plots our simulated points in red and blue. The one that\nare not in the training set are faded. We also plot the linear separation\nestimated by the SVM.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot(a, b, score, selected):\n    plt.xlabel('Accuracy {}%'.format(int(score * 100)), fontsize=10)\n\n    # Plot not selected first in low alpha, then selected\n    for l, s in [(0, False), (1, False), (0, True), (1, True)]:\n        alpha = 1. if s else 0.3\n        color = 'tomato' if l == 0 else 'royalblue'\n        mask = np.logical_and(selected == s, l == y)\n        plt.scatter(X[mask, 0], X[mask, 1], c=color, alpha=alpha)\n\n    # Plot the separation margin of the SVM\n    x_bounds = np.array([np.min(X[:, 0]), np.max(X[:, 0])])\n    plt.plot(x_bounds, a * x_bounds + b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Core active learning experiment\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nAs presented in the introduction, this loop represents the active learning\nexperiment. At each iteration, the model is learned on all labeled data to\nmeasure its performance. Then, the model is inspected to find out the samples\non which its confidence is low. This is done through cardinal samplers.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "samplers = [\n    ('Random', RandomSampler(batch_size=batch_size, random_state=0)),\n    ('Lowest confidence', ConfidenceSampler(model, batch_size))\n]\n\nplt.figure(figsize=(10, 4))\n\nfor i, (sampler_name, sampler) in enumerate(samplers):\n    # We force having one sample in each class for the init\n    init_idx = [np.where(y == 0)[0][0], np.where(y == 1)[0][0]]\n\n    mask = np.zeros(n, dtype=bool)\n    indices = np.arange(n)\n    mask[init_idx] = True\n\n    for j in range(n_iter):\n        model.fit(X[mask], y[mask])\n        sampler.fit(X[mask], y[mask])\n        w = model.coef_[0]\n        \n        plt.subplot(len(samplers), n_iter, i * n_iter + j + 1)\n        plot(-w[0] / w[1], - model.intercept_[0] / w[1], model.score(X, y), mask.copy())\n\n        selected = sampler.select_samples(X[~mask])\n        mask[indices[~mask][selected]] = True\n\n        if j == 0:\n            plt.ylabel(sampler_name)\n        plt.axis('tight')\n        plt.gca().set_xticks(())\n        plt.gca().set_yticks(())\n        if i == 0:\n            plt.gca().set_title('Iteration {}'.format(j), fontsize=10)\n\nplt.tight_layout()\nplt.subplots_adjust(top=0.86)\nplt.gcf().suptitle('Classification accuracy of random and uncertainty active learning on simulated data', fontsize=12)\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}