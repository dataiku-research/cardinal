{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Active Learning on Digit Recognition and Metrics\n\nIn this example, we run an experiment on real data and show how active\nlearning can be monitored, given that in real life scenario there is no access\nto the ground truth of the test set. Based on these metrics, we\nidentify two phases during this active learning experiment and define\na custom query sampler that takes advantage of this.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Those are the necessary imports and initializations\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\nimport numpy as np\n\nfrom sklearn.datasets import load_digits\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import pairwise_distances\n\nfrom cardinal.uncertainty import ConfidenceSampler\nfrom cardinal.clustering import KMeansSampler\nfrom cardinal.random import RandomSampler\nfrom cardinal.plotting import plot_confidence_interval\nfrom cardinal.base import BaseQuerySampler\nfrom cardinal.metrics import ContradictionMonitor\nfrom cardinal.utils import ActiveLearningSplitter\n\nnp.random.seed(7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The parameters of this experiment are:  \n\n* `batch_size` is the number of samples that will be annotated and added to\n  the training set at each iteration,\n* `n_iter` is the number of iterations in our simulation\n\nWe use the digits dataset and a RandomForestClassifier as model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "batch_size = 20\nn_iter = 20\n\nX, y = load_digits(return_X_y=True)\nX /= 255.\nn_classes = 10\nn_samples = X.shape[0]\n\nmodel = RandomForestClassifier()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experimental Metrics\n\nWe define a first metric based on contradictions. It has been observed that\nthe number of samples on which the model changes his prediction from one\niteration to the other is correlated to the improvement of accuracy. We\nwant to verify this. Since the number of label prediction changes can be\ncoarse, we use the absolute difference in prediction probabilities.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def compute_contradiction(previous_proba, current_proba):\n    return np.abs(current_proba - previous_proba).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define a second metric based on the distance between already labeled\nsamples and our test set. The goal of this metric is measure how well our\ntest set has been explored by our query sampling method so far. We expect\nuncertainty sampling to explore the sample space located *nearby* the\ndecision boundary and show poor exploration property.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def compute_exploration(X_selected, X_test):\n    return pairwise_distances(X_selected, X_test).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A New Custom Sampler\n\nLet's imagine what an ideal query sampler could look like by making the\nbest of the samplers we already know.  \nUncertainty sampling is certainly an appealing method but in a previous\nexample we have shown that it lacks exploration and can stay stuck in a\nlocal minimum for a while. On the other hand, K-Means sampling explore the\nspace well but will probably fail at fine tuning our prediction model. In\nthat context, it seems reasonable to first explore the sample\nspace, say by using a KMeansSampler, and at some point shift to\nan exploitation mode where we fine tune our model using UncertaintySampler.\nWe define an Adaptive sampler that does exactly this.\n\nAs a heuristic, let us say that 5 samples per class should be enough\nexploration. We set the sampler to explore until it has 50 samples and\nthen switch to exploitation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class AdaptiveQuerySampler(BaseQuerySampler):\n    def __init__(self, exploration_sampler, exploitation_sampler,\n                 exploration_budget):\n        self.exploration_sampler = exploration_sampler\n        self.exploitation_sampler = exploitation_sampler\n        self.exploration_budget = exploration_budget\n        self.sampler = None\n    \n    def fit(self, X_train, y_train):\n        if X_train.shape[0] <= self.exploration_budget:\n            self.sampler = self.exploration_sampler.fit(X_train, y_train)\n        else:\n            self.sampler = self.exploitation_sampler.fit(X_train, y_train)\n        return self\n    \n    def select_samples(self, X):\n        return self.sampler.select_samples(X)\n\n\nadaptive_sampler = AdaptiveQuerySampler(\n    KMeansSampler(batch_size),  # Exploration\n    ConfidenceSampler(model, batch_size),  # Exploitation\n    n_classes * 5\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Core Active Learning Experiment\n\nWe now perform the experiment. We compare our adaptive model to random,\npure exploration, and pure exploitation. We also monitor the metrics\ndefined above.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "samplers = [\n    ('Adaptive', adaptive_sampler),\n    ('Lowest confidence', ConfidenceSampler(model, batch_size)),\n    ('KMeans', KMeansSampler(batch_size)),\n    ('Random', RandomSampler(batch_size)),\n]\n\nfigure_accuracies = plt.figure().number\nfigure_contradictions = plt.figure().number\nfigure_explorations = plt.figure().number\n\nfor i, (sampler_name, sampler) in enumerate(samplers):\n    \n    all_accuracies = []\n    all_contradictions = []\n    all_explorations = []\n\n    for k in range(10):\n        splitter = ActiveLearningSplitter.train_test_split(n_samples, test_size=500, random_state=k)\n        splitter.initialize_with_random(batch_size, at_least_one_of_each_class=y[splitter.train], random_state=k)\n\n        accuracies = []\n        contradictions = ContradictionMonitor()\n        explorations = []\n\n        previous_proba = None\n\n        # The classic active learning loop\n        for j in range(n_iter):\n            model.fit(X[splitter.selected], y[splitter.selected])\n\n            # Record metrics\n            accuracies.append(model.score(X[splitter.test], y[splitter.test]))\n            explorations.append(compute_exploration(X[splitter.selected], X[splitter.test]))\n            contradictions.accumulate(splitter.selected.sum(),\n                                      model.predict_proba(X[splitter.test]))\n\n            sampler.fit(X[splitter.selected], y[splitter.selected])\n            selected = sampler.select_samples(X[splitter.non_selected])\n            splitter.add_batch(selected)\n\n        all_accuracies.append(accuracies)\n        all_explorations.append(explorations)\n        all_contradictions.append(contradictions.get()['contradictions'])\n    \n    x_data = np.arange(10, batch_size * (n_iter - 1) + 11, batch_size)\n\n    plt.figure(figure_accuracies)\n    plot_confidence_interval(x_data, all_accuracies, label=sampler_name)\n\n    plt.figure(figure_contradictions)\n    plot_confidence_interval(x_data[1:], all_contradictions,\n                             label=sampler_name)\n\n    plt.figure(figure_explorations)\n    plot_confidence_interval(x_data, all_explorations, label=sampler_name)\n\nplt.figure(figure_accuracies)\nplt.xlabel('Labeled samples')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.tight_layout()\n\nplt.figure(figure_contradictions)\nplt.xlabel('Labeled samples')\nplt.ylabel('Contradictions')\nplt.legend()\nplt.tight_layout()\n\nplt.figure(figure_explorations)\nplt.xlabel('Labeled samples')\nplt.ylabel('Exploration score')\nplt.legend()\nplt.tight_layout()\n\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Discussion\n\n### Accuracies\n\nIn all the figures above, the vertical red line indicates when the adaptive\nmethod switches from exploration to exploitation.\n\nWe first look at accuracy. As expected, KMeansSampler, a purely exploration\nmethod is the best at the beginning but becomes as performant as random\nwith time. Uncertainty sampling also behaves as expected by starting poorly and\nthen becoming the best method.\n\nOur AdaptiveQuerySampler combines the performance of both approaches to have the\nbest performance!\n\n### Contradictions\n\nContradictions measures how much, when trained on new samples, a model agrees with\nits past predictions. A perfectly stable model should have consistent predictions \ngiven new samples so we expect this measure to converge toward 0 over time. Looking \nat the red Random curve and the green Clustering, it seems like contradictions are\ninversely proportional to accuracy. However, we notice an interesting trend when\nthe adaptive model switches from exploration to exploitation. In fact, the number\nof contradictions seems to stall a bit and join the orange Uncertainty curve.\n\nThis effect is probably due to samples that are \"far\" from the training set. \nIf the Uncertainty focuses on a given region of the sample space, it is likely that\nthe samples far from those regions will be subject to more variability. By better\nexploring the space, KMeans sampling is less sensitive to these changes.\nIn the end, contradictions seems related to accuracy but contradictions weighted\nby exploration may be a better proxy for accuracy. This subject remains opened\nto further research.\n\n### Exploration Scores\n\nThe exploration curve also displays interesting trends. Since those are\ndistance, a good exploration method will have a low score. As expected,\nexploration-based methods have the lowest scores. In particular,\nKMeansSampler starts by decreasing and then goes up. At the same time, its\nperformance starts stalling. This shift happens incidentally at the same\ntime as our adaptive method shifts its method. This is obviously not random!\nThis exploration metric can be used to decide when to change method.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}