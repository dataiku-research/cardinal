{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Starting an Active Learning experiment\n\nEach active learning experiment starts with zero labeled samples. In that\ncase, no supervised query strategy can be used. It is common to select the\nfirst batch of samples at random however it is proposed in\n[Diverse mini-batch Active Learning](https://arxiv.org/abs/1901.05954)\nto use a clustering approach for the first batch.\nThis example shows how to perform this.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We start with necessary imports and initializations\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\nimport numpy as np\nfrom time import time\n\nfrom sklearn.datasets import load_digits\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.cluster import KMeans\n\nfrom cardinal.uncertainty import MarginSampler\nfrom cardinal.clustering import KMeansSampler\nfrom cardinal.random import RandomSampler\nfrom cardinal.plotting import plot_confidence_interval\nfrom cardinal.base import BaseQuerySampler\nfrom cardinal.zhdanov2019 import TwoStepKMeansSampler\nfrom cardinal.utils import ActiveLearningSplitter\n\nnp.random.seed(7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The parameters of this experiment are:\n\n* ``batch_size`` is the number of samples that will be annotated and added to\n  the training set at each iteration,\n* ``n_iter`` is the number of iterations in the simulation.\n\nWe use the digits dataset and a RandomForestClassifier.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "batch_size = 45\nn_iter = 8\n\nX, y = load_digits(return_X_y=True)\nX /= 255.\nn_samples = X.shape[0]\n\nmodel = RandomForestClassifier()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Core Active Learning Experiment\n\nWe now compare our class sampler to Zhdanov, a simpler KMeans approach and, \nof course, random. For each method, we measure the time spent at each iteration \nand we plot the accuracy depending on the size of the labeled pool but also time spent.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "starting_samplers = [\n    ('KMeans', KMeansSampler(batch_size)),\n    ('Random', RandomSampler(batch_size)),\n]\n\nsamplers = [\n    ('Zhdanov', TwoStepKMeansSampler(5, model, batch_size)),\n    ('Margin', MarginSampler(model, batch_size)),\n]\n\n#figure_accuracies = plt.figure().number\n\n\nfor starting_sampler_name, starting_sampler in starting_samplers:\n    for sampler_name, sampler in samplers:\n    \n        all_accuracies = []\n\n        for k in range(4):\n            splitter = ActiveLearningSplitter.train_test_split(n_samples, test_size=500, random_state=k)\n            splitter.initialize_with_random(batch_size, at_least_one_of_each_class=y[splitter.train], random_state=k)\n\n            accuracies = []\n\n            # The classic active learning loop\n            for j in range(n_iter):\n                model.fit(X[splitter.selected], y[splitter.selected])\n\n                # Record metrics\n                accuracies.append(model.score(X[splitter.test], y[splitter.test]))\n\n                t0 = time()\n                sampler.fit(X[splitter.selected], y[splitter.selected])\n                selected = sampler.select_samples(X[splitter.non_selected])\n                splitter.add_batch(selected)\n\n            all_accuracies.append(accuracies)\n    \n        x_data = np.arange(10, batch_size * (n_iter - 1) + 11, batch_size)\n        plot_confidence_interval(x_data, all_accuracies, label='{} + {}'.format(starting_sampler_name, sampler_name))\n\n\nplt.xlabel('Labeled samples')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.tight_layout()\n\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Discussion\n\nFrom this experiment on a subset of MNIST, we confirm the observation of the\noriginal work: Selecting the first batch of samples using a K-Means sampler\nincreases the accuracy for the first few batches but does not improve the\nfinal result, at least on this dataset and with the query strategies we\nexplore.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}