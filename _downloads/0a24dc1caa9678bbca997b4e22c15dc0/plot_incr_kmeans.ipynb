{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Incremental KMeans\n\nIn an active learning setting, the trade-off between exploration and\nexploitation plays a central role. Exploration, or diversity, is\nusually enforced using coresets or, more simply, a clustering algorithm.\nKMeans is therefore used to select samples that are spread across the\ndataset in each batch. However, to the best of our knowledge, maintaining diversity\nthroughout the whole experiment is something that has not been considered.\n\nBy allowing to start the optimization with fixed cluster centers, our\nIncremental KMeans allows for an optimal exploration of the space since\nsamples are less likely to be selected nearby an already selected sample.\n\nThis notebook introduces the principle behind Incremental KMeans and runs\na small benchmark on generated data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import MiniBatchKMeans\nfrom sklearn.metrics import pairwise_distances_argmin_min\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom cardinal.kmeans import IncrementalMiniBatchKMeans\nfrom cardinal.clustering import MiniBatchKMeansSampler, IncrementalMiniBatchKMeansSampler\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nfrom scipy.stats import ttest_rel\nfrom cardinal.utils import ActiveLearningSplitter\nimport numpy as np\n\n# Inertia\n# ^^^^^^^\n#\n# Throughout this notebook, we will use the inertia metric, it is the\n# euclidean distance from a set of points to a set of center points.\n\n\ndef inertia(data, centers):\n    return (pairwise_distances_argmin_min(data, centers)[1] ** 2).sum()\n\n\n\n# Active learning like-experiment\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n#\n# Before digging into the behaviors proper to incremental KMeans, we test\n# it in an active learning experiment on simulated data.\n\nX, y, centers = make_blobs(n_samples=10000, centers=100, return_centers=True,\n                           n_features=512, random_state=2)\n\nclf = RandomForestClassifier()\n\nkmeans_inertia = []\nkmeans_accuracy = []\nsampler = MiniBatchKMeansSampler(10)\n\nidx = ActiveLearningSplitter.train_test_split(10000, test_size=.2, random_state=0)\nidx.initialize_with_indices(np.arange(10))\n\nfor i in range(10):\n    selected = sampler.fit(X[idx.selected]).select_samples(X[idx.non_selected])\n    idx.add_batch(selected)\n    kmeans_inertia.append(inertia(X[idx.test], X[idx.selected]))\n\n    clf.fit(X[idx.selected], y[idx.selected])\n    kmeans_accuracy.append(clf.score(X[idx.test], y[idx.test]))\n    \n\nikmeans_inertia = []\nikmeans_accuracy = []\nsampler = IncrementalMiniBatchKMeansSampler(10, random_state=0)\n\nidx = ActiveLearningSplitter.train_test_split(10000, test_size=.2, random_state=0)\nidx.initialize_with_indices(np.arange(10))\n\nfor i in range(10):\n\n    selected = sampler.fit(X[idx.selected]).select_samples(X[idx.non_selected])\n\n    idx.add_batch(selected)\n    ikmeans_inertia.append(inertia(X[idx.test], X[idx.selected]))\n\n    clf.fit(X[idx.selected], y[idx.selected])\n    ikmeans_accuracy.append(clf.score(X[idx.test], y[idx.test]))\n\n\nplt.plot(range(10), kmeans_accuracy, label='KMeans')\nplt.plot(range(10), ikmeans_accuracy, label='Incr. KMeans')\nplt.ylabel('Accuracy \u2500\u2500\u2500\u2500')\nplt.legend(loc=6)\nplt.xlabel('Iteration')\n\nplt.gca().twinx()\nplt.gca().set_prop_cycle(None)\nplt.plot(range(10), kmeans_inertia, '--')\nplt.plot(range(10), ikmeans_inertia, '--')\nplt.ylabel('Inertia \u2576\u2576\u2576\u2576')\n\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this simple exemple, we see that Incremental KMeans has a decisive\nadvantage over KMeans because it is able to gradually explore the space of\nsamples.\n\n## Analysis of Incremental KMeans\n\nLet us first define the parameters of our experiment. We will generate a\ndataset from 8 clusters and we will try to keep 4 clusters fixed. We do\nthis example in 2 dimensions for visualization purposes.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_clusters = 8\nn_fixed_clusters = 4\n\nX, y, centers = make_blobs(centers=n_clusters, return_centers=True,\n                           n_features=2, random_state=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define plotting functions. This function allows to plot the data colored\nby clusters, and allows to displays fixed cluster centers in red and regular\ncenters in blue. We use it to display the ground truth.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_clustering(label, y_pred, centers, fixed_centers=[], inertia=None):\n    colors = ['C{}'.format(i) for i in y_pred]\n    plt.scatter(X[:, 0], X[:, 1], c=colors)\n    plt.scatter(centers[:, 0], centers[:, 1], c='blue', edgecolors='k',\n                marker='P', s=200)\n    if len(fixed_centers) > 0:\n        plt.scatter(fixed_centers[:, 0], fixed_centers[:, 1],\n                    c='red', edgecolors='k', marker='P', s=200)\n    plt.xticks([])\n    plt.yticks([])\n    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n    if inertia:\n        label = label + \", inertia={0:0.2f}\".format(inertia)\n    plt.title(label)\n    plt.show()\n\nplot_clustering('Ground truth', y, centers, inertia=inertia(X, centers))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We run a regular MiniBatchKMeans. KMeans would be more suited for this kind\nof small dataset but we are aiming at using Increment KMeans on large\ndatasets so its implementation relies on MiniBatchKMeans.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "kmeans = MiniBatchKMeans(n_clusters=8, random_state=2)\nkmeans.fit(X)\nplot_clustering('KMeans', kmeans.predict(X), kmeans.cluster_centers_,\n                inertia=kmeans.inertia_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now consider that we are aware of 4 of the 8 clusters. We fix them in\nthe IncrementalMiniBatchKMeans so that they are strictly enforced.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ikmeans = IncrementalMiniBatchKMeans(n_clusters=8, random_state=2)\nikmeans.fit(X, fixed_cluster_centers=centers[:n_fixed_clusters])\nplot_clustering(\n    'Incremental KMeans', ikmeans.predict(X),\n    centers[n_fixed_clusters:],\n    fixed_centers=centers[:n_fixed_clusters],\n    inertia=ikmeans.inertia_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this basic experiment, we see that the clusters fixed in Incremental\nKMeans have stayed fixed. We see that the inertia is a little bit lower but\nthis is an effect dependent on the seed. Overall there is no significant\ndifference between the two methods on this toy example.\n\n## Experiments in higher dimension\n\nWe now want to see the effect of some parameters on the inertia. For this\npurpose, we repeat the same experiment on a larger dataset with higher\ndimensions. We explore only a few aspects of the algorithm but our general\nevaluation function can be used to dig deeper in the algorithm.\n\nThe parameter `reassignement_ratio` is a parameter of the MiniBatchKMeans\nthat controls the ratio of centers of high inertia randomly reassigned\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def evaluate(n_samples, n_repeat, n_features, n_blobs, n_clusters,\n             n_fixed_clusters, reassignment_ratio):\n    inertiae = []\n    for i in range(n_repeat):\n        X, y, centers = make_blobs(\n            centers=n_blobs,\n            return_centers=True,\n            n_features=n_features,\n            random_state=i)\n        clus = IncrementalMiniBatchKMeans(\n            n_clusters=n_clusters,\n            reassignment_ratio=reassignment_ratio\n        )\n        kwargs = dict()\n        kwargs['fixed_cluster_centers'] = None\n        if n_fixed_clusters > 0:\n            kwargs['fixed_cluster_centers'] = centers[:n_fixed_clusters]\n        clus.fit(X, **kwargs)\n        inertiae.append(clus.inertia_)\n    return inertiae\n \n\ndef plot(xlabel, ylabel, values, inertiae):\n    arr = np.asarray(inertiae)\n    for i in range(arr.shape[1]):\n        plt.scatter(values, arr[:, i], alpha=.4, c='gray')\n    plt.plot(values, np.mean(arr, axis=1), c='r')\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Number of fixed cluster centers\n\nIn this experiment, we generate data from 10 clusters and see how the number\nof fixed clusters impacts the inertia.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "study_value = []\nstudy_inert = []\n\nfor i in range(10):\n    study_value.append(i)\n    inert = evaluate(5000, 20, 20, 10, 10, i, 0.01)\n    study_inert.append(inert)\n    \nplot('Number of fixed center clusters /10', 'Mean inertia over 100 runs',\n     study_value, study_inert)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We observe that inertia is proportional to the number of fixed clusters.\nThis happens because fixed clusters are not in a minimum inertia position\nto start with, so fixing them prevents inertia to be optimized.\n\n# Reassignment ratio\n\nWe expect the fixed clusters to \"get in the way\" of clusters trying to move\naround to reach their cluster. Fortunately, the KMeans++ initialization\nusually takes care of this by positioning initialization centers in the best\nareas.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "study_value = []\nstudy_inert = []\n\nfor i in [0., 0.01, 0.05, 0.1, 0.15, 0.2, 0.3, 0.5]:\n    study_value.append(i)\n    inert = evaluate(5000, 20, 20, 10, 10, 5, i)\n    study_inert.append(inert)\n    \nplot('Reassignment ratio', 'Mean inertia over 100 runs',\n     study_value, study_inert)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the end, we observe that the reassignment ratio has absolutely no impact\non the intertia which is reassuring.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}