{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Measuring hard to classify samples\n\nIn this example, we run an experiment on real data and try to relate\nthe amount of *hard-to-classify* samples in the training set with the\nsampler performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\nimport numpy as np\n\nfrom sklearn.datasets import load_digits\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.metrics import pairwise_distances\n\nfrom cardinal.uncertainty import MarginSampler\nfrom cardinal.clustering import TwoStepIWKMeansSampler, TwoStepGMMSampler\nfrom cardinal.zhdanov2019 import TwoStepKMeansSampler\nfrom cardinal.random import RandomSampler\nfrom cardinal.plotting import plot_confidence_interval\nfrom cardinal.utils import ActiveLearningSplitter\n\nnp.random.seed(7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The parameters of this experiment are:  \n\n* `batch_size` is the number of samples that will be annotated and added to\n  the training set at each iteration,\n* `n_iter` is the number of iterations in our simulation\n\nWe use the digits dataset and a RandomForestClassifier as model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "batch_size = 20\nn_iter = 20\n\nX, y = load_digits(return_X_y=True)\nX /= 255.\nn_classes = 10\n\nmodel = RandomForestClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Core Active Learning Experiment\n\nWe now perform the experiment. We compare our adaptive model to random,\npure exploration, and pure exploitation. We also monitor the metrics\ndefined above.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "samplers = [\n    ('Random', lambda clf: RandomSampler(batch_size)),\n    ('Margin', lambda clf: MarginSampler(clf, batch_size)),\n    ('WKMeans', lambda clf: TwoStepKMeansSampler(10, clf, batch_size)),\n    ('IWKMeans', lambda clf: TwoStepIWKMeansSampler(10, clf, batch_size)),\n    ('GMM', lambda clf: TwoStepGMMSampler(10, clf, batch_size))\n]\n\nall_splitters = dict()\nall_accuracies = dict()\nall_selected_hardness = dict()\nall_batch_hardness = dict()\n\nfor i, (sampler_name, sampler_gen) in enumerate(samplers):\n    \n    splitters = []\n    all_selected_hardness[sampler_name] = []\n    all_batch_hardness[sampler_name] = []\n    all_accuracies[sampler_name] = []\n\n    for train_idx, test_idx in RepeatedStratifiedKFold(n_splits=2, n_repeats=2, random_state=i).split(X, y):\n        splitter = ActiveLearningSplitter(X.shape[0], test_index=test_idx)\n        splitter.initialize_with_random(n_classes, at_least_one_of_each_class=y[splitter.train])\n        X_test, y_test = X[splitter.test], y[splitter.test]\n        clf_trained_on_test = model().fit(X_test, y_test)\n\n        accuracies = []\n        batch_hardness = []\n        selected_hardness = []\n\n        # The classic active learning loop\n        for j in range(n_iter):\n            clf = model()\n            clf.fit(X[splitter.selected], y[splitter.selected])\n            selected_hardness.append(1. - clf_trained_on_test.predict_proba(X[splitter.selected])[np.arange(splitter.selected.sum()), y[splitter.selected]].mean())\n            batch_hardness.append(1. - clf_trained_on_test.predict_proba(X[splitter.batch])[np.arange(splitter.batch.sum()), y[splitter.batch]].mean())\n\n            # Record metrics\n            accuracies.append(clf.score(X_test, y_test))\n\n            sampler = sampler_gen(clf)\n            sampler.fit(X[splitter.selected], y[splitter.selected])\n            selected = sampler.select_samples(X[splitter.non_selected])\n            splitter.add_batch(selected)\n\n        all_selected_hardness[sampler_name].append(selected_hardness)\n        all_batch_hardness[sampler_name].append(batch_hardness)\n        all_accuracies[sampler_name].append(accuracies)\n\n# Plot accuracies\nplt.figure()\nx_data = np.cumsum([batch_size] * n_iter)\nfor sampler in all_accuracies:\n    plot_confidence_interval(x_data, all_accuracies[sampler], label=sampler)\nplt.title('Accuracies')\nplt.legend()\n\nplt.figure()\nx_data = np.cumsum([batch_size] * n_iter)\nfor sampler in all_selected_hardness:\n    plot_confidence_interval(x_data, all_selected_hardness[sampler], label=sampler)\nplt.title('Selected hardness')\nplt.legend()\n\nplt.figure()\nx_data = np.cumsum([batch_size] * n_iter)\nfor sampler in all_batch_hardness:\n    plot_confidence_interval(x_data, all_batch_hardness[sampler], label=sampler)\nplt.title('Batch hardness')\nplt.legend()\n\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}