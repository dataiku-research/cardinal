{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Isolated cluster and bad init\n\nThe iterative process of Active Learning may induce corner cases that\nare proper to Active Learning. In this case, we look at what happens if\nsome data has an isolated cluster where no point is selected at init.\n\nThis cluster may be totally overlooked by some samplers that do not\nexplore the dataset well enough through diversity or representativity\nsampling.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n\nimport numpy as np\nfrom sklearn.datasets import make_blobs\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom matplotlib import pyplot as plt\n\nfrom cardinal.utils import ActiveLearningSplitter\nfrom cardinal.uncertainty import ConfidenceSampler\nfrom cardinal.random import RandomSampler\nfrom cardinal.clustering import KMeansSampler\nfrom cardinal.plotting import smooth_lines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We simulate two classes. Class 0 is simply a large blob. Class 1 is composed\nof a large blob and an isolated cluster located aside. In order to generate\nthis cluster, the data for this class is seperated into two blobs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X, blob = make_blobs([500, 400, 100], centers=[(2, 0), (-2, 0), (4, 5)], cluster_std=[1.0, 1.0, 0.3], random_state=0)\nbatch_size = 10\nclf = LogisticRegression()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now create the active learning experiment. We use cardinal's splitter\nto handles indices. For the initialisation, we only sample data from the\nfirst two blobs. This simulates an unlucky initialization where no data\nfrom the isolated cluster is selected.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "init_spl = ActiveLearningSplitter(X.shape[0], test_size=0.2, stratify=blob, random_state=0)\ninit_spl.add_batch(np.hstack([\n    np.where(blob[init_spl.train] == 0)[0][:batch_size],\n    np.where(blob[init_spl.train] == 1)[0][:batch_size]\n]))\nleft_out_y = (blob[init_spl.test] == 2)\ny = blob.copy()\ny[blob == 2] = 1\n\nplt.scatter(X[:, 0], X[:, 1], c=['C{}'.format(i) for i in y], alpha=.3)\nplt.scatter(X[init_spl.selected, 0], X[init_spl.selected, 1], facecolors='none', edgecolors='r', linewidth=2, label='Init batch')\nplt.gca().add_patch(plt.Circle((4, 5), 1.2, color='r', fill=False, linestyle='dashed', linewidth=2))\nplt.text(4, 3.2, 'Isolated cluster', ha='center', c='r')\n\nplt.legend()\nplt.axis('off')\n\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This function runs the experiment. It is a class active learning setting.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def evaluate(name, sampler, g_ax, ic_ax):\n    spl = deepcopy(init_spl)\n    g_acc = []\n    ic_acc = []\n\n    for _ in range(10):\n        clf.fit(X[spl.selected], y[spl.selected])\n        sampler.fit(X[spl.selected], y[spl.selected])\n        spl.add_batch(sampler.select_samples(X[spl.non_selected]))\n        g_acc.append(accuracy_score(y[spl.test], clf.predict(X[spl.test])))\n        ic_acc.append(accuracy_score(y[spl.test][left_out_y], clf.predict(X[spl.test][left_out_y])))\n    \n    g_ax.plot(np.arange(10), g_acc, label=name)\n    ic_ax.plot(np.arange(10), ic_acc, label=name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now display the results for 3 very common samplers. You may observe that\nthe confidence sampling completely ignores the isolated cluster since it\nis designed to focus on the existing decision boundary. Random sampling has\na 15% chance of picking a sample in this cluster during the experiment. By\ndesign, KMeans sampling will always select samples in the isolated cluster!\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.figure()\nglobal_ax = plt.gca()\nplt.ylabel('Global accuracy')\nplt.xlabel('Iteration')\n\nplt.figure()\nisolated_cluster_ax = plt.gca()\nplt.ylabel('Isolated cluster accuracy')\nplt.xlabel('Iteration')\n\nevaluate('Confidence Sampler', ConfidenceSampler(clf, batch_size=batch_size, assume_fitted=True), global_ax, isolated_cluster_ax)\nevaluate('Random Sampler', RandomSampler(batch_size=batch_size, random_state=0), global_ax, isolated_cluster_ax)\nevaluate('KMeans Sampler', KMeansSampler(batch_size=batch_size), global_ax, isolated_cluster_ax)\n\nglobal_ax.legend()\nsmooth_lines(axis=global_ax, k=2)\nisolated_cluster_ax.legend()\nsmooth_lines(axis=isolated_cluster_ax, k=2)\n\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}