{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Lowest confidence vs. Random sampling\n\nThe simplest way to convince ourselves that active learning actually\nworks is to first test it on simulated data. In this example, we will\ngenerate a simple classification task and see how active learning allows\nto converge faster toward the best result.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Those are the necessary imports and initializations\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\nimport numpy as np\n\nfrom sklearn.datasets import make_blobs\nfrom sklearn.svm import SVC\n\nfrom cardinal.random import RandomSampler\nfrom cardinal.uncertainty import ConfidenceSampler\nfrom cardinal.utils import ActiveLearningSplitter\n\nnp.random.seed(8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our simulated data is composed of 2 clusters that are very close to each other\nbut linearly separable. We use as simple SVM classifier as it is a basic classifer. \n\nThe parameters of this experiment are:  \n\n* ``n`` is the number of points in the sumulated data,\n* ``batch_size`` is the number of samples that will be annotated and added to\n  the training set at each iteration,\n* ``n_iter`` is the number of iterations in our simulation\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n = 30\nbatch_size = 2\nn_iter = 6\n\nX, y = make_blobs(n_samples=n, centers=2,\n                  random_state=0, cluster_std=0.80)\n\nmodel = SVC(kernel='linear', C=1E10, probability=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This helper function plots our simulated points in red and blue. The one that\nare not in the training set are faded. We also plot the linear separation\nestimated by the SVM.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot(a, b, score, selected):\n    plt.xlabel('Accuracy {}%'.format(int(score * 100)), fontsize=10)\n\n    # Plot not selected first in low alpha, then selected\n    for l, s in [(0, False), (1, False), (0, True), (1, True)]:\n        alpha = 1. if s else 0.3\n        color = 'tomato' if l == 0 else 'royalblue'\n        mask = np.logical_and(selected == s, l == y)\n        plt.scatter(X[mask, 0], X[mask, 1], c=color, alpha=alpha)\n\n    # Plot the separation margin of the SVM\n    x_bounds = np.array([np.min(X[:, 0]), np.max(X[:, 0])])\n    plt.plot(x_bounds, a * x_bounds + b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Core Active Learning Experiment\n\nAs presented in the introduction, this loop represents the active learning\nexperiment. At each iteration, the model learn on all labeled data to\nmeasure its performance. The model is then inspected to find out the samples\non which its confidence is the lowest. This is done through cardinal samplers.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "samplers = [\n    ('Random', RandomSampler(batch_size=batch_size, random_state=0)),\n    ('Lowest confidence', ConfidenceSampler(model, batch_size))\n]\n\nplt.figure(figsize=(10, 4))\n\nfor i, (sampler_name, sampler) in enumerate(samplers):\n    splitter = ActiveLearningSplitter(X.shape[0])\n    splitter.initialize_with_random(2, at_least_one_of_each_class=y)\n\n    for j in range(n_iter):\n        model.fit(X[splitter.selected], y[splitter.selected])\n        sampler.fit(X[splitter.selected], y[splitter.selected])\n        w = model.coef_[0]\n        \n        plt.subplot(len(samplers), n_iter, i * n_iter + j + 1)\n        plot(-w[0] / w[1], - model.intercept_[0] / w[1], model.score(X, y), splitter.selected.copy())\n\n        selected = sampler.select_samples(X[splitter.non_selected])\n        splitter.add_batch(selected)\n\n        if j == 0:\n            plt.ylabel(sampler_name)\n        plt.axis('tight')\n        plt.gca().set_xticks(())\n        plt.gca().set_yticks(())\n        if i == 0:\n            plt.gca().set_title('Iteration {}'.format(j), fontsize=10)\n\nplt.tight_layout()\nplt.subplots_adjust(top=0.86)\nplt.gcf().suptitle('Classification accuracy of random and uncertainty active learning on simulated data', fontsize=12)\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}