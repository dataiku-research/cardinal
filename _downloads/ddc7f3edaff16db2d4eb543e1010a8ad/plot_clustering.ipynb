{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Active Learning on Digit Recognition with Clustering-based Sampling\n\nWe here compare several clustering-based Active Learning approaches and\nin particular explore the trade-off between speed and performance.\nFor this purpose, we use two standard clustering: a simple KMeans and a MiniBatchKMeans.\nWe compare both of them to Zhdanov's approach proposed in\n`Diverse mini-batch Active Learning <https://arxiv.org/abs/1901.05954>`_.\nThe author introduced a two-step procedure that\nfirst select samples using uncertainty and then performs a KMeans.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Those are the necessary imports and initializations\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\nimport numpy as np\nfrom time import time\n\nfrom sklearn.datasets import load_digits\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.cluster import KMeans\n\nfrom cardinal.uncertainty import MarginSampler\nfrom cardinal.clustering import KMeansSampler\nfrom cardinal.random import RandomSampler\nfrom cardinal.plotting import plot_confidence_interval\nfrom cardinal.base import BaseQuerySampler\nfrom cardinal.zhdanov2019 import TwoStepKMeansSampler\nfrom cardinal.utils import pad_with_random\n\nnp.random.seed(7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The parameters of this experiment are:\n\n* ``batch_size`` is the number of samples that will be annotated and added to\n  the training set at each iteration,\n* ``n_iter`` is the number of iterations in the simulation.\n\nWe use the digits dataset and a RandomForestClassifier.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "batch_size = 45\nn_iter = 10\n\nX, y = load_digits(return_X_y=True)\nX /= 255.\n\nmodel = RandomForestClassifier()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A new Custom Sampler\n\nZhdanov preselects ``beta * batch_size`` samples using uncertainty sampling\nand then uses KMeans to cluster the data in `batch_size clusters and select\nthe samples closest to the cluster centers. By doing this, a diverse set of\nsample with high uncertainty is selected.\n\nWe hypothetize that inversing these steps can also be an interesting\napproach. For that, we first perform a KMeans clustering, and then select\nwithin each cluster the most uncertain samples.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class KMeansClassSampler(BaseQuerySampler):\n    def __init__(self, model, n_classes, batch_size):\n        self.clustering = KMeans(n_clusters=n_classes)\n        self.confidence_sampler = MarginSampler(model, batch_size // n_classes)\n        super().__init__(batch_size)\n    \n    def fit(self, X_train, y_train):\n        self.confidence_sampler.fit(X_train, y_train)\n        return self\n    \n    def select_samples(self, X):\n        self.clustering.fit(X)\n        all_samples = set()\n        for label in np.unique(self.clustering.labels_):\n            indices = np.arange(X.shape[0])\n            mask = np.zeros(X.shape[0], dtype=bool)\n            mask[self.clustering.labels_ == label] = True\n            selected = self.confidence_sampler.select_samples(X[mask])\n            all_samples.update(indices[mask][selected])\n        return pad_with_random(np.asarray(list(all_samples)), self.batch_size,\n                               0, X.shape[0])\n\n\nclass_sampler = KMeansClassSampler(model, 15, 45)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Core Active Learning Experiment\n\nWe now compare our class sampler to Zhdanov, a simpler KMeans approach and, \nof course, random. For each method, we measure the time spent at each iteration \nand we plot the accuracy depending on the size of the labeled pool but also time spent.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "samplers = [\n    ('ClassSampler', class_sampler),\n    ('Zhdanov', TwoStepKMeansSampler(5, model, batch_size)),\n    ('KMeans', KMeansSampler(batch_size)),\n    ('Random', RandomSampler(batch_size)),\n]\n\nfigure_accuracies = plt.figure().number\nfigure_execution_times = plt.figure().number\n\n\nfor sampler_name, sampler in samplers:\n    \n    all_accuracies = []\n    all_execution_times = []\n\n    for k in range(10):\n        X_train, X_test, y_train, y_test = \\\n            train_test_split(X, y, test_size=500, random_state=k)\n\n        accuracies = []\n        execution_times = []\n\n        # For simplicity, we start with one sample of each class\n        _, selected = np.unique(y_train, return_index=True)\n\n        # We use binary masks to simplify some operations\n        mask = np.zeros(X_train.shape[0], dtype=bool)\n        indices = np.arange(X_train.shape[0])\n        mask[selected] = True\n\n        # The classic active learning loop\n        for j in range(n_iter):\n            model.fit(X_train[mask], y_train[mask])\n\n            # Record metrics\n            accuracies.append(model.score(X_test, y_test))\n\n            t0 = time()\n            sampler.fit(X_train[mask], y_train[mask])\n            selected = sampler.select_samples(X_train[~mask])\n            mask[indices[~mask][selected]] = True\n            execution_times.append(time() - t0)\n\n        all_accuracies.append(accuracies)\n        all_execution_times.append(execution_times)\n    \n    x_data = np.arange(10, batch_size * (n_iter - 1) + 11, batch_size)\n    x_time = np.cumsum(np.mean(all_execution_times, axis=0))\n\n    plt.figure(figure_accuracies)\n    plot_confidence_interval(x_data, all_accuracies, label=sampler_name)\n\n    plt.figure(figure_execution_times)\n    plot_confidence_interval(x_time, all_execution_times, label=sampler_name)\n\n\nplt.figure(figure_accuracies)\nplt.xlabel('Labeled samples')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.tight_layout()\n\nplt.figure(figure_execution_times)\nplt.xlabel('Cumulated execution time')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.tight_layout()\n\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Discussion\n\n### Accuracies\n\nLet's start by focusing on the figure showing accuracy depending on the number\nof labeled samples. We observe that regular KMeans is slightly better\nat the beginning, probably because it is pure diversity and that exploration\nis best at the beginning. After that, Zhdanov's method clearly takes over\nthe other methods.\n\n### Execution Times\n\nExecution time is usually not critical in active learning as the query\nsampling time is negligible compared to labeling time. However, a significant\ndecrease of query sampling time could allow to train more often and thus\nincrease the accuracy of the model faster.\n\nIn the plot showing accuracy depending on execution time, we first see a red\nbar which is the random sampling. This method is almost instantaneous. We\nalso see that the domination of Zhdanov is even more clear. It is because\nthe two-step approach allows to greatly reduce the number of samples on which\nthe KMeans is performed. The difference is small here because we have a small\ndataset but on a bigger one, the win can be significant. One can also\nconsider using mini-batch KMeans from scikit-learn that is faster.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}